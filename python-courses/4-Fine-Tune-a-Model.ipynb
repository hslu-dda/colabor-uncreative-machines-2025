{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune a Model\n",
    "\n",
    "A fine-tuned [distilgpt2](https://huggingface.co/distilbert/distilgpt2) model for generating manga-style quotes, trained on a [curated dataset](https://www.kaggle.com/datasets/tarundalal/anime-quotes) of anime and manga quotes.\n",
    "\n",
    "Needs Tensorflow 2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print(f\"Tensorflow {tf.__version__}\")\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM # to load pretrained models\n",
    "\n",
    "import pandas as pd # for data processing\n",
    "import re # for text processing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Pretrained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"  # Use a TensorFlow-supported model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Character</th>\n",
       "      <th>Anime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People’s lives don’t end when they die, it end...</td>\n",
       "      <td>Itachi Uchiha</td>\n",
       "      <td>Naruto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you don’t take risks, you can’t create a fu...</td>\n",
       "      <td>Monkey D Luffy</td>\n",
       "      <td>One Piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you don’t like your destiny, don’t accept it.</td>\n",
       "      <td>Naruto Uzumaki</td>\n",
       "      <td>Naruto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you give up, that’s when the game ends.</td>\n",
       "      <td>Mitsuyoshi Anzai</td>\n",
       "      <td>Slam Dunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All we can do is live until the day we die. Co...</td>\n",
       "      <td>Deneil Young</td>\n",
       "      <td>Uchuu Kyoudai or Space Brothers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Quote         Character  \\\n",
       "0  People’s lives don’t end when they die, it end...     Itachi Uchiha   \n",
       "1  If you don’t take risks, you can’t create a fu...    Monkey D Luffy   \n",
       "2   If you don’t like your destiny, don’t accept it.    Naruto Uzumaki   \n",
       "3       When you give up, that’s when the game ends.  Mitsuyoshi Anzai   \n",
       "4  All we can do is live until the day we die. Co...      Deneil Young   \n",
       "\n",
       "                             Anime  \n",
       "0                           Naruto  \n",
       "1                        One Piece  \n",
       "2                           Naruto  \n",
       "3                        Slam Dunk  \n",
       "4  Uchuu Kyoudai or Space Brothers  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/AnimeQuotes.csv\")\n",
    "print(\"Dataset loaded successfully\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People’s lives don’t end when they die it ends when they lose faith.',\n",
       " 'If you don’t take risks you can’t create a future']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to clean text by removing unwanted punctuation\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\"”!?,…]', '', text)  # Remove specific punctuation\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    # print(text)\n",
    "    return text\n",
    "\n",
    "# Extract the \"Quote\" column from the DataFrame\n",
    "quotes = df['Quote']\n",
    "\n",
    "# Convert all values in the column to strings (in case they are not already)\n",
    "quotes = quotes.astype(str)\n",
    "\n",
    "# Apply text cleaning function\n",
    "quotes = quotes.apply(clean_text)\n",
    "\n",
    "# Convert the column into a Python list (so it can be processed further)\n",
    "quotes = quotes.tolist()\n",
    "\n",
    "quotes[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "- The process of tokenization transforms a sentence into a list of tokens, from `\"People’s lives don’t end when they die it ends when they lose faith.\"` to `1️⃣ ['People', '’s', 'lives', 'don', '’t', 'end', 'when', 'they', 'die', 'it', 'ends', 'when', 'they', 'lose', 'faith', '<PAD>', '<PAD>', ..., '<PAD>']`.\n",
    "\n",
    "### 4.1 Padding\n",
    "\n",
    "- Padding is added since all sentences used for training need to have the same length. The model does not learn from <PAD> tokens. Instead, the attention mask marks them as “ignore” areas.\n",
    "- Padding can either be defined manually by adding `padding=\"max_length\", max_length=50,` to the tokenizer, or by setting `padding=True` for automatially setting the padding for every batch.\n",
    "- Ideally, the max-padding value should be slightly above the 95th percentile of sentence lengths to cover most data while avoiding excessive padding. Use this if effeciency is important. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 73\n",
      "Mean length: 24.07\n",
      "95th percentile: 49.0\n"
     ]
    }
   ],
   "source": [
    "# Optional\n",
    "# Calculate 95th percentile for efficient padding\n",
    "quote_lengths = [len(tokenizer.encode(q)) for q in quotes]\n",
    "\n",
    "# Print summary stats\n",
    "print(f\"Max length: {max(quote_lengths)}\")\n",
    "print(f\"Mean length: {np.mean(quote_lengths):.2f}\")\n",
    "print(f\"95th percentile: {np.percentile(quote_lengths, 95)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Creating the Tokens and Attention Mask \n",
    "\n",
    "When the Tokenizer returns a tensor dictionary, it consist of two array `input_ids` and `attention_mask`. \n",
    "- input_ids → The numerical token representation of the sentences. \n",
    "    - Example: \"Life is an adventure.\" → [72, 318, 281, 18299, 13, 50256, 50256]\n",
    "- attention_mask → A binary tensor: \n",
    "    - 1 → Real tokens (actual words).\n",
    "    - 0 → Padding tokens, ignored by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(121, 73), dtype=int32, numpy=\n",
       "array([[50256, 50256, 50256, ...,  4425,  4562,    13],\n",
       "       [50256, 50256, 50256, ...,  2251,   257,  2003],\n",
       "       [50256, 50256, 50256, ...,  2453,   340,    13],\n",
       "       ...,\n",
       "       [50256, 50256, 50256, ...,  2119,   284,  1663],\n",
       "       [50256, 50256, 50256, ...,   534,  7401, 29955],\n",
       "       [50256, 50256, 50256, ...,  1566,   340,  9457]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(121, 73), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a TensorFlow tensor dictionary\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_quotes = tokenizer(\n",
    "    quotes, truncation=True, padding=True, return_tensors=\"tf\"\n",
    ")\n",
    "tokenized_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Shifting Labels\n",
    "\n",
    "Casual Language Models like distilGPT2 predict **the next token** not the current one.\n",
    "\n",
    "Below is a comparison of **before and after shifting**:\n",
    "\n",
    "| Step | Input Token (Training Input) | ❌ Labels Without Shifting (Incorrect) | ✅ Labels With Shifting (Correct) |\n",
    "|------|------------------------------|----------------------------------------|----------------------------------|\n",
    "| 1️⃣  | `Life`                       | `Life` ❌ (Wrong, should predict `\"is\"`) | `is` ✅ |\n",
    "| 2️⃣  | `is`                         | `is` ❌ (Wrong, should predict `\"an\"`) | `an` ✅ |\n",
    "| 3️⃣  | `an`                         | `an` ❌ (Wrong, should predict `\"adventure\"`) | `adventure` ✅ |\n",
    "| 4️⃣  | `adventure`                  | `adventure` ❌ (Wrong, should predict `\".\"`) | `.` ✅ |\n",
    "| 5️⃣  | `.`                          | `.` ❌ (Wrong, should predict `<|endoftext|>`) | `<|endoftext|>` ✅ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to properly set up dataset\n",
    "def map_fn(input_ids, attention_mask):\n",
    "    labels = tf.concat([input_ids[:, 1:], tf.fill((tf.shape(input_ids)[0], 1), tokenizer.pad_token_id)], axis=1)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tokenized_quotes[\"input_ids\"], tokenized_quotes[\"attention_mask\"])\n",
    ").batch(BATCH_SIZE).map(map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-Tune Pre-Trained Model with Custom Data Set\n",
    "\n",
    "Some notes on interpreting `loss`:\n",
    "- Below 2.0 → Model is learning well.\n",
    "- Between 1.0 - 1.5 → Good text generation capability.\n",
    "- Below 1.0 → Model is highly trained and very accurate.\n",
    "- However, if the loss is too low (~0.5 or lower), the model might be overfitting (just memorizing the training data instead of generalizing).\n",
    "- The drop in loss should be smooth and consistent, meaning the model is learning without sudden overfitting.\n",
    "\n",
    "Then, test the output.\n",
    "- If the output looks good, no need for further training!\n",
    "- If the output is still a bit repetitive, train for 2-3 more epochs (EPOCHS = 7 or 8) with a lower learning rate (learning_rate=3e-5) to refine the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 17s 823ms/step - loss: 3.6913\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 13s 821ms/step - loss: 1.9983\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 13s 819ms/step - loss: 1.7581\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 13s 827ms/step - loss: 1.5630\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 13s 821ms/step - loss: 1.4648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x32b8d6f90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow.keras as keras  # Make sure you are using TensorFlow's Keras\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "\n",
    "# manually define loss function\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 5\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Save the model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model-export/tokenizer_config.json',\n",
       " './model-export/special_tokens_map.json',\n",
       " './model-export/vocab.json',\n",
       " './model-export/merges.txt',\n",
       " './model-export/added_tokens.json',\n",
       " './model-export/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./model-export/\")\n",
    "tokenizer.save_pretrained(\"./model-export/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate a Manga-Style Quote\n",
    "\n",
    "input_ids – The encoded input prompt from which the model will generate text.\n",
    "- Example: input_ids = tokenizer.encode(\"A warrior never\", return_tensors=\"tf\")\n",
    "\n",
    "max_length=50 – The maximum number of tokens (words + punctuation) the model will generate before stopping.\n",
    "- Increase for longer responses (e.g., max_length=100 for full paragraphs).\n",
    "- Decrease for shorter quotes (e.g., max_length=30).\n",
    "\n",
    "temperature=0.7 – Controls randomness in word selection.\n",
    "- Lower values (e.g., 0.3) make the output more predictable and deterministic.\n",
    "- Higher values (e.g., 1.0) make the output more creative and diverse.\n",
    "\n",
    "top_k=50 – Limits word selection to the top 50 most probable words at each step.\n",
    "- Lower values (e.g., top_k=10) make the output more focused.\n",
    "- Higher values increase diversity but can lead to randomness.\n",
    "\n",
    "top_p=0.9 – Enables nucleus sampling, which selects words from the smallest group of high-probability choices that together add up to 90% probability.\n",
    "- If top_p=1.0, the model considers all possible words (more unpredictable).\n",
    "- If top_p=0.5, the model limits selection to only the most likely words (more controlled).\n",
    "\n",
    "do_sample=True – Enables sampling instead of greedy decoding, which improves creativity.\n",
    "- If False, the model will always pick the highest-probability token (more robotic responses).\n",
    "- If True, the model will randomly sample from the probability distribution (more natural responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is a very important tool for us to make sure that we are doing the right thing.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Life is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    temperature=0.7, # Increase randomness, 0.7\n",
    "    top_k=50, # Reduce selection pool for better variety, 50\n",
    "    top_p=0.6, # Adjust nucleus sampling for controlled diversity, 0.6\n",
    "    repetition_penalty=1.2,  # Reduce repetitive phrases\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2 #Prevents the model from generating EOS too soon if a phrase has already appeared\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True).strip('\"”’“‘')\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Best of Quotes \n",
    "\n",
    "3 Epochs\n",
    "- Life is the most important thing in life.\n",
    "- Life is a waste of time,’s hard work and effort. It will not be the best thing you can do\n",
    "- Life is a world of pain.\n",
    "- Life is a place to be. We’re going down and on, we can continue\n",
    "- Life is the perfect moment of life.\n",
    "\n",
    "5 Epochs\n",
    "- Life is not a thing. But it’s the beginning of all things in life\n",
    "- Life is not a matter of fate. It’s an act of destiny to be loved and peace for you all the\n",
    "- Life is the only thing you can do.’s what it takes to live with your enemies and overcome them all!\n",
    "- Life is a game of luck. You’ll never lose sight to your enemies and gain the most important thing you can\n",
    "- Life is the most important thing to do. You’ll never forget it\n",
    "- Life is not a game of luck. It’s the outcome that makes you happy and strong but it doesn't mean what your opponent wins! If there are no problems to overcome then they can always win with patience or hard work as well\n",
    "- Life is a beautiful place to live. But it’s not the right time for you!\n",
    "- Life is not the end of all things but a journey. The beginning will be as long and beautiful as you can find it. You are free to live your way through life’s path. Your destiny is what makes up for it all. True, but when.\n",
    "\n",
    "- Art is the same thing as a dream.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
